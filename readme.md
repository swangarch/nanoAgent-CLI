# NanoAgent-CLI

### A minimalist CLI coding agent in < 100 lines of python code
---

With fewer than 100 lines of Python, this repo implements a minimalist CLI AI agent from scratch, without using agent frameworks. It turns an LLM into a CLI-based coding AI agent.

Yes, it can do vibe coding.

The agent is tested using the open-source model **GLM-4.7**, it accepts also other models with OpenAI API including remote and local LLMs, and it can autonomously:
- Write web pages
- Create terminal-based games
- Execute shell commands
- Manipulate files
- Git commit & push
- And more

Some tests results are placed in /examples, they are all generated by nanoAgent-CLI.

## Background

This project is inspired by:

- **ReAct** (Reason + Act) pattern
- The idea of **we can do anything with bash command.**
- The idea that as foundation models improve in reasoning, **complex agent frameworks become less necessary**, simple interaction loop is enough. 
- **Prompt engineering**. The project combines prompt and code. It uses carefully designed prompts, followed by checks, to constrain and guide the model's behavior.

## Core Ideas
- Minimalism first: fewer abstractions, more clarity
- Transparent behavior: you see and prove every command before execution
- Validation of model compliance to instructions, with feedback to enable self-correction.
- This agent is implemented in a **ReAct** pattern loop:
    - User sends task
    - Model reasons about the task
    - Model proposes a bash command
    - User approves or rejects execution
    - System feeds execution result back to the model
    - Loop continues until <TASK_DONE> is emitted


## Getting Started

### Requirements
- Python 3.8+
- Linux / macOS / WSL for windows
- Bash shell
- Python Dependencies

```python
pip install openai python-dotenv
```

Or use the provided setup script and virtual environment:
```bash
bash setup.sh
source venv/bin/activate
```

### Configuration
Create a .env file in the project root:

#### Examples
Using remote model (e.g. OpenAI)
```
BASE_URL="https://api.openai.com/v1"
API_KEY="sk-xxxx"
MODEL="gpt-4o-mini"
```

Using local model (e.g. Ollama)
```
BASE_URL="http://localhost:11434/v1"
API_KEY="ollama"
MODEL="qwen2.5:7b"
```
### Run agent

```bash
python nanoAgent-CLI.py
```

You will see an interactive CLI like:
```
[Agent]: How can I help you?
[User]: (wait input)
```
Now you can ask agent to do things. 

eg:
```
[User]:  Generate me a snake game in terminal, using python, put it in snake folder.
```

or 
```
[User]:  Write me a portfolio website to showcase my photos, put it in site folder.
```

or 
```
[User]:  Organise the files for me, put the files related to calculator to where it should go.
```

or 
```
[User]:  Write a commit message, and push to remote repo.
```

When the model proposes a command, you must manually approve it:
```
[[Execute task] -> (ls -la ...) y/n? ] >
```
If you do not enter y, the execution will be canceled and the model will be notified.

The model will do the task step-by-step, it will continue to output command, until the task is done, you can break the task with Ctrl+C,
and give model further instruction.

### How It Works
- Using system prompt to restrict model behavior
- User gives instruction
- The model start reasoning
- The model outputs bash commands inside fenced blocks
- Commands are extracted via regex
- Only **one bash block is allowed per response**
- Execution results are fed back to the model as system feedback
- The task ends when the model outputs: <TASK_DONE>, to wait new instructions.

---

## Why NanoAgent?

To understand and create a minimalist LLM Agent:

- You don’t need LangChain
- You don’t need complex function calling protocol
- You don’t need 10 layers of abstraction
- You don't need blackbox tools

In the react pattern, you just need: **A main loop, a sub loop, a model, and a shell.**
And it works.

---

## Limitations

- No sandboxing
- No permission system
- No tool schema validation
- Performance heavily depends on the base model’s reasoning ability
- Different models may behave **very differently**，some model doesn't strictly follow system prompt

## Security Notice

**Caution**

Allowing an LLM to generate and execute bash commands is **inherently risky**.

- Always review commands before execution
- Never run this agent with elevated privileges
- Do not expose it to untrusted users
- Do not use it on production systems

This project is for **educational and experimental purposes**.

---

## License

This project is under MIT License.